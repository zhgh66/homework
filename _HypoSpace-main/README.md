<div align="center">

# ğŸ”¬ HypoSpace

### *Evaluating LLM Creativity as Set-Valued Hypothesis Generators under Underdetermination*

[![Paper](https://img.shields.io/badge/ğŸ“„_Paper-arXiv-b31b1b.svg)](https://arxiv.org)
[![Python](https://img.shields.io/badge/Python-3.8+-3776AB.svg?logo=python&logoColor=white)](https://www.python.org)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

---

### ğŸ¯ Three Domains â€¢ Three Metrics â€¢ Infinite Insights

</div>

<p align="center">
  <img src="figs/overview.png" alt="Overview" height="220">
  &nbsp;&nbsp;&nbsp;
  <img src="figs/comp.png" alt="Model Comparison" height="220">
</p>

<p align="center">
  <img src="figs/task.png" alt="Task Illustration" width="60%">
</p>

<div align="center">

### ğŸ§¬ Causal Graphs â€¢ ğŸ“¦ 3D Reconstruction â€¢ ğŸ”€ Boolean Logic

</div>

---

## ğŸ“– About

> **TL;DR**: HypoSpace evaluates how well LLMs generate *diverse sets* of valid hypotheses in underdetermined scientific problems, not just single correct answers.

### The Challenge

As language models are increasingly used in scientific workflows, evaluating their ability to propose **sets of explanations**â€”not just a single correct answerâ€”becomes critical. Many scientific problems are **underdetermined**: multiple, mechanistically distinct hypotheses are consistent with the same observations.

### Our Solution

We introduce **HypoSpace**, a diagnostic suite that treats LLMs as samplers of finite hypothesis sets and measures three complementary indicators:

| Metric | Symbol | What It Measures |
|--------|--------|------------------|
| **ğŸ¯ Validity** | *V* | Precision of proposals consistent with observations |
| **âœ¨ Uniqueness** | *U* | Non-redundancy among proposals |
| **ğŸ“ˆ Recovery** | *R* | Coverage of the enumerated admissible set |

### Three Structured Domains

We instantiate HypoSpace in three domains with deterministic validators and exactly enumerated hypothesis spaces:

1. **ğŸ§¬ Causal Graphs** â€” from perturbations
2. **ğŸ“¦ 3D Voxel Reconstruction** â€” gravity-constrained from top-down projections  
3. **ğŸ”€ Boolean Genetic Interactions** â€” logical function discovery

### Key Findings

Across instruction-tuned and reasoning-focused models, **Validity** often remains high while **Uniqueness** and **Recovery** degrade as the admissible space grows, revealing **mode collapse** that is invisible to correctness-only metrics.

> ğŸ’¡ HypoSpace offers a controlled probeâ€”rather than a leaderboardâ€”for methods that explicitly explore and cover admissible explanation spaces.

---

## ğŸ“ Repository Structure

```
ğŸ“‚ HypoSpace/
â”‚
â”œâ”€â”€ ğŸ“¦ 3d/                                  â† 3D Voxel Reconstruction Domain
â”‚   â”œâ”€â”€ ğŸ”§ generate_3d_dataset_complete.py  â€¢ Dataset generator
â”‚   â”œâ”€â”€ ğŸš€ run_3d_benchmark.py              â€¢ Benchmark runner
â”‚   â”œâ”€â”€ ğŸ“š modules/                         â€¢ LLM interface & models
â”‚   â”‚   â”œâ”€â”€ llm_interface.py
â”‚   â”‚   â””â”€â”€ models.py
â”‚   â””â”€â”€ âš™ï¸  config/
â”‚       â””â”€â”€ config_gpt4o.yaml               â€¢ Configuration file
â”‚
â”œâ”€â”€ ğŸ”€ boolean/                             â† Boolean Genetic Interactions
â”‚   â”œâ”€â”€ ğŸ”§ boolean_dataset.py               â€¢ Dataset generator
â”‚   â”œâ”€â”€ ğŸš€ boolean_benchmark.py             â€¢ Benchmark runner
â”‚   â”œâ”€â”€ ğŸ“š modules/
â”‚   â”‚   â”œâ”€â”€ llm_interface.py
â”‚   â”‚   â””â”€â”€ models.py
â”‚   â””â”€â”€ âš™ï¸  config/
â”‚       â””â”€â”€ config_gpt4o.yaml
â”‚
â””â”€â”€ ğŸ§¬ causal/                              â† Causal Graph Discovery
    â”œâ”€â”€ ğŸ”§ generate_causal_dataset.py       â€¢ Dataset generator (small)
    â”œâ”€â”€ ğŸ”§ generate_causal_dataset_for_large.py  â€¢ Dataset generator (large)
    â”œâ”€â”€ ğŸš€ run_causal_benchmark.py          â€¢ Benchmark runner
    â”œâ”€â”€ ğŸ“š modules/
    â”‚   â”œâ”€â”€ llm_interface.py
    â”‚   â””â”€â”€ models.py
    â””â”€â”€ âš™ï¸  config/
        â””â”€â”€ config_gpt4o.yaml
```

---

## ğŸš€ Quick Start

### Step 1ï¸âƒ£: Configure Your LLM

Edit the YAML config files in each domain's `config/` folder:

**What you can customize:**
- ğŸ¤– LLM provider and model
- ğŸŒ¡ï¸ Temperature settings
- ğŸ“‚ Output paths
- ğŸ’¾ Checkpoint directories

**Example:** `config/config_gpt4o.yaml`

```yaml
llm:
  type: openrouter              # Options: openai, anthropic, openrouter
  models:
    openrouter: "openai/gpt-4o"
  api_keys:
    openrouter: "your-api-key"  # âš ï¸ Replace with your actual API key
  temperature: 0.7              # 0.0 = deterministic, 1.0 = creative

benchmark:
  checkpoint: "checkpoints"     # Resume interrupted runs
  verbose: true                 # Print detailed logs
  output_pattern: "results/{dataset_name}_{model}.json"
```

### Step 2ï¸âƒ£: Generate Datasets

Each domain has its own dataset generator. Here are examples for all three:

<details>
<summary><b>ğŸ§¬ Causal Graphs</b> (click to expand)</summary>

```bash
cd causal
python generate_causal_dataset.py \
  --nodes 3 \
  --seed 33550336 \
  --output "datasets/node03/n3_all_observations.json"
```

**Parameters:**
- `--nodes`: Number of nodes in graphs (3, 4, 5, etc.)
- `--seed`: Random seed for reproducibility
- `--output`: Path to save dataset JSON

</details>

<details>
<summary><b>ğŸ“¦ 3D Voxel Reconstruction</b> (click to expand)</summary>

```bash
cd 3d
python generate_3d_dataset_complete.py \
  --grid-size 3 \
  --max-height 3 \
  --max-blocks 1 \
  --fixed \
  --seed 33550336 \
  --output "datasets/3d_grid3_h3.json"
```

**Parameters:**
- `--grid-size`: Grid dimensions (e.g., 3 for 3Ã—3)
- `--max-height`: Maximum structure height
- `--max-blocks`: Maximum number of blocks in top view
- `--fixed`: If set, generate only structures with exactly max-blocks blocks, else from 1 to max-blocks
- `--output`: Output file path

</details>

<details>
<summary><b>ğŸ”€ Boolean Logic</b> (click to expand)</summary>

```bash
cd boolean
python boolean_dataset.py \
  --operators basic \
  --max-depth 2 \
  --output 'datasets/boolean_2var.json' \
  --seed 33550336 \
```

**Parameters:**
- `--operators`: Allowed Boolean operators: choices=['basic', 'extended', 'full']
- `--max-depth`: Maximum expression depth
- `--output`: Output JSON file

</details>

### Step 3ï¸âƒ£: Run Benchmarks

Run the benchmark for your chosen domain:

<details>
<summary><b>ğŸ§¬ Causal Benchmark</b></summary>

```bash
cd causal
python run_causal_benchmark.py \
  --dataset "datasets/node03/n3_all_observations.json" \
  --config "config/config_gpt4o.yaml" \
  --n-samples 30 \
  --query-multiplier 1.0 \
  --seed 33550336
```

**Run in background with logging:**
```bash
nohup python -u run_causal_benchmark.py \
  --dataset "datasets/node03/n3_all_observations.json" \
  --config "config/config_gpt4o.yaml" \
  --n-samples 30 \
  --query-multiplier 1.0 \
  --seed 33550336 > logs/causal_gpt4o.log 2>&1 &
```

</details>

<details>
<summary><b>ğŸ“¦ 3D Benchmark</b></summary>

```bash
cd 3d
python run_3d_benchmark.py \
  --dataset "datasets/3d_grid3_h3.json" \
  --config "config/config_gpt4o.yaml" \
  --n-samples 30 \
  --query-multiplier 1.0 \
  --seed 33550336
```

</details>

<details>
<summary><b>ğŸ”€ Boolean Benchmark</b></summary>

```bash
cd boolean
python boolean_benchmark.py \
  --dataset "datasets/boolean_2var.json" \
  --config "config/config_gpt4o.yaml" \
  --n-samples 30 \
  --query-multiplier 1.0 \
  --seed 33550336
```

</details>

**Common Parameters:**
- `--dataset`: Path to generated dataset
- `--config`: Configuration YAML file
- `--n-samples`: Number of observation sets to evaluate
- `--query-multiplier`: Multiplier for queries per task
- `--seed`: Random seed for reproducibility

### Step 4ï¸âƒ£: Analyze Results

Results are automatically saved as JSON files in the `results/` directory.

**What's included:**

```json
{
  "metadata": {
    "model": "openai/gpt-4o",
    "dataset": "causal_n3",
    "n_samples": 30,
    "timestamp": "2025-10-17T12:00:00"
  },
  "aggregate_metrics": {
    "mean_validity": 0.92,      // ğŸ¯ How many proposals are valid
    "mean_uniqueness": 0.78,    // âœ¨ How diverse are the proposals
    "mean_recovery": 0.65,      // ğŸ“ˆ Coverage of solution space
    "std_validity": 0.08,
    "std_uniqueness": 0.12,
    "std_recovery": 0.15
  },
  "results": [/* detailed per-sample results */]
}
```

**Understanding the Metrics:**

| Metric | Range | Good Score | Interpretation |
|--------|-------|------------|----------------|
| ğŸ¯ **Validity** | 0-1 | > 0.90 | Model proposes correct hypotheses |
| âœ¨ **Uniqueness** | 0-1 | > 0.80 | Model avoids redundant proposals |
| ğŸ“ˆ **Recovery** | 0-1 | > 0.80 | Model explores solution space well |

---

## ğŸ“Š Supported Models

| Provider | Example Models | Config Type |
|----------|----------------|-------------|
| **OpenAI** | GPT-4o, GPT-4-turbo, GPT-3.5 | `openai` |
| **OpenRouter** | Any model via OpenRouter | `openrouter` |

---

## ğŸ“ Citation

If you use HypoSpace in your research, please cite:

```bibtex
@article{hypospace2025,
  title={HypoSpace: Evaluating LLM Creativity as Set-Valued Hypothesis Generators under Underdetermination},
  author={Your Name and Collaborators},
  journal={arXiv preprint arXiv:XXXX.XXXXX},
  year={2025}
}
```

---

## ğŸ“„ License

This project is released under the MIT License.

---

<div align="center">

**Built with â¤ï¸ for scientific discovery**

â­ Star us on GitHub â€¢ ğŸ› Report issues â€¢ ğŸ’¡ Suggest features

</div>
